{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression model from library Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of linear regression model w: [[3.01021115 3.99177204]]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(1000, 1)\n",
    "Y = 4 + 3*X + 0.2*np.random.randn(1000, 1)\n",
    "\n",
    "one = np.ones((X.shape[0], 1))\n",
    "X_bar = np.concatenate((X, one), axis = 1)\n",
    "\n",
    "model = linear_model.LinearRegression(fit_intercept=False).fit(X_bar, Y)\n",
    "w = model.coef_\n",
    "\n",
    "print('Result of linear regression model w:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential(x_bar, y, w):\n",
    "    N = x_bar.shape[0]\n",
    "    return (1/N)*X_bar.T.dot(X_bar.dot(w) - y) \n",
    "\n",
    "def cost(x_bar, y, w):\n",
    "    N = x_bar.shape[0]\n",
    "    return 0.5*(1/N)*np.linalg.norm(y - x_bar.dot(w), 2)**2\n",
    "\n",
    "def gradient_descent(x_bar, y, w0, eta, differential, iterations = 1000):\n",
    "    w = [w0]\n",
    "    for it in range(iterations):\n",
    "        w_new = w[-1] - eta*differential(x_bar, y, w[-1])\n",
    "        if np.linalg.norm(differential(x_bar, y, w_new))/len(w_new) < 1e-5:\n",
    "            break\n",
    "        w.append(w_new)   \n",
    "    \n",
    "    return np.array(w), it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(x_bar, y, w, cost):\n",
    "    eps = 1e-4\n",
    "    g = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        w_p = w.copy()\n",
    "        w_n = w.copy()\n",
    "        w_p[i] += eps \n",
    "        w_n[i] -= eps\n",
    "        g[i] = (cost(x_bar, y, w_p) - cost(x_bar, y, w_n))/(2*eps)\n",
    "    return g \n",
    "\n",
    "def check_gradient(w, cost, differential):\n",
    "    w = np.random.rand(w.shape[0], w.shape[1])\n",
    "    grad1 = differential(X_bar, Y, w)\n",
    "    grad2 = numerical_gradient(X_bar, Y, w, cost)\n",
    "    return True if np.linalg.norm(grad1 - grad2) < 1e-6 else False \n",
    "\n",
    "print('Checking gradient formula...', check_gradient(np.random.rand(2, 1), cost, differential))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result tuning from Batch gradient descent with learning rate 0.05 after 25 epochs at iteration 1/1 w: [3.09, 3.54]\n",
      "Result tuning from Stochastic gradient descent with learning rate 0.05 after 3 epochs at iteration 240/1000 w: [3.06, 3.97]\n",
      "Result tuning from Mini-batch gradient descent with learning rate 0.05 after 25 epochs at iteration 8/8 w: [3.01, 3.99]\n"
     ]
    }
   ],
   "source": [
    "def n_batch_GD_NAG(x_bar, y, w_init, eta, epochs = 100, batch_size = 32, gamma = 0.9):\n",
    "\n",
    "    def n_batch_differential(x_bar, y, w, i, random_id, batch_size, n):\n",
    "        current_id = random_id[i*batch_size : i*batch_size + n]\n",
    "        xi_n = np.array(x_bar[current_id, :]) # (n,2)\n",
    "        yi_n = np.array(y[current_id]) # (n,1)\n",
    "        N = xi_n.shape[0]\n",
    "        return (1/N)*xi_n.T.dot(xi_n.dot(w) - yi_n)\n",
    "\n",
    "    w = [w_init]\n",
    "    v_old = np.zeros_like(w_init)\n",
    "\n",
    "    w_last_check = w_init\n",
    "    iter_check_w = 10\n",
    "    N = X.shape[0]\n",
    "    count = 0\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle data \n",
    "        random_id = np.random.permutation(N)\n",
    "        times = int(N/batch_size) if (N % batch_size == 0) else int(N/batch_size) + 1\n",
    "        for i in range(times):\n",
    "            count += 1\n",
    "            n = batch_size if (i + 1) <= N/batch_size else int(N % batch_size)\n",
    "            v_new = gamma*v_old + eta*n_batch_differential(x_bar, y, w[-1] - gamma*v_old, i, random_id, batch_size, n)\n",
    "            w_new = w[-1] - v_new\n",
    "            w.append(w_new)\n",
    "            v_old = v_new \n",
    "            if count % iter_check_w == 0:              \n",
    "                if np.linalg.norm(w_new - w_last_check)/len(w_init) < 1e-3:                                    \n",
    "                    return w[-1], epoch, i, times - 1\n",
    "                w_last_check = w_new\n",
    "    return w[-1], epoch, times - 1, times - 1\n",
    "\n",
    "def Batch_GD(x_bar, y, w_init, eta, epochs = 100):\n",
    "    N = x_bar.shape[0]\n",
    "    w, epoch, iteration, total = n_batch_GD_NAG(x_bar, y, w_init, eta, epochs, N)\n",
    "    return w, epoch, iteration, total\n",
    "\n",
    "def Stochastic_GD(x_bar, y, w_init, eta, epochs = 100):\n",
    "    w, epoch, iteration, total = n_batch_GD_NAG(x_bar, y, w_init, eta, epochs, 1, 0.5)\n",
    "    return w, epoch, iteration, total\n",
    "\n",
    "def MiniBatch_GD(x_bar, y, w_init, eta, epochs = 100, batch_size = 32):\n",
    "    w, epoch, iteration, total = n_batch_GD_NAG(x_bar, y, w_init, eta, epochs, batch_size)\n",
    "    return w, epoch, iteration, total\n",
    "\n",
    "w_init = np.array([[2], [1]])\n",
    "learning_rate = 0.05\n",
    "epochs = 25\n",
    "\n",
    "w_BGD, epoch_BGD, iteration, total = Batch_GD(X_bar, Y, w_init, learning_rate, epochs)\n",
    "print(\"Result tuning from Batch gradient descent with learning rate %.2f after %d epochs at iteration %d/%d w: [%.2f, %.2f]\" %(learning_rate, epoch_BGD + 1, iteration + 1, total + 1, w_BGD[0], w_BGD[1]))\n",
    "\n",
    "w_SGD, epoch_SGD, iteration, total = Stochastic_GD(X_bar, Y, w_init, learning_rate, epochs)\n",
    "print(\"Result tuning from Stochastic gradient descent with learning rate %.2f after %d epochs at iteration %d/%d w: [%.2f, %.2f]\" %(learning_rate, epoch_SGD + 1, iteration + 1, total + 1, w_SGD[0], w_SGD[1]))\n",
    "\n",
    "w_MiniBatchGD, epoch_MiniBatchGD, iteration, total = MiniBatch_GD(X_bar, Y, w_init, learning_rate, epochs, 128)\n",
    "print(\"Result tuning from Mini-batch gradient descent with learning rate %.2f after %d epochs at iteration %d/%d w: [%.2f, %.2f]\" %(learning_rate, epoch_MiniBatchGD + 1, iteration + 1, total + 1, w_MiniBatchGD[0], w_MiniBatchGD[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
