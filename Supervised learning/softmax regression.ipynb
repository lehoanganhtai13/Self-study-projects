{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy import sparse \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly generate data \n",
    "N = 10 # number of training sample \n",
    "d = 2 # data dimension \n",
    "C = 3 # number of classes \n",
    "\n",
    "X = np.random.randn(N, d)\n",
    "y = np.random.randint(1, 4, (N,1))\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(y)\n",
    "Y = enc.transform(y).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients size: (3, 2)\n",
      "Checking gradient formula... True\n"
     ]
    }
   ],
   "source": [
    "def softmax(Z):\n",
    "    return (np.exp(Z).T/np.sum(np.exp(Z), axis=1)).T # (dim(C,N)/dim(N)).T = (N,C)\n",
    "\n",
    "def cost(xi, yi, w):\n",
    "    ai = softmax(xi.dot(w.T))\n",
    "    return -np.sum(yi*np.log(ai))\n",
    "\n",
    "def SGD(xi, yi, w):\n",
    "    ai = softmax(xi.dot(w.T))\n",
    "    ei = ai - yi\n",
    "    return ei.T.dot(xi)\n",
    "\n",
    "def check_gradient(X, Y, cost, differential):\n",
    "\n",
    "    def numerical_gradient(x, y, w, cost):\n",
    "        eps = 1e-6\n",
    "        g = np.zeros_like(w)\n",
    "        for i in range(w.shape[0]):\n",
    "            for j in range(w.shape[1]):\n",
    "                w_p = w.copy()\n",
    "                w_n = w.copy()\n",
    "                w_p[i, j] += eps \n",
    "                w_n[i, j] -= eps\n",
    "                g[i,j] = (cost(x, y, w_p) - cost(x, y, w_n))/(2*eps)\n",
    "        return g \n",
    "\n",
    "    W = np.random.randn(Y.shape[1], X.shape[1])\n",
    "    print('Coefficients size:', W.shape)\n",
    "    grad1 = differential(X, Y, W)\n",
    "    grad2 = numerical_gradient(X, Y, W, cost)\n",
    "    return True if np.linalg.norm(grad1 - grad2) < 1e-6 else False \n",
    "\n",
    "print('Checking gradient formula...', check_gradient(X, Y, cost, SGD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients found by built-in model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-18.89840473,   9.13241615],\n",
       "       [  7.14007442,  -5.14470842],\n",
       "       [  9.87068953,  -4.33744232]])"
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_regression(X, y, eta = 1, tol = 1e-4, early_stopping = True, epochs = 50000):\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    encoder.fit(y)\n",
    "    Y = encoder.transform(y).toarray()\n",
    "\n",
    "    W_init = np.random.randn(Y.shape[1], X.shape[1])\n",
    "    W = [W_init]    \n",
    "    it = 0\n",
    "    N = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    \n",
    "    check_w_after = 10\n",
    "    for epoch in range(1,epochs):\n",
    "        # shuffle data \n",
    "        mix_id = np.random.permutation(N)\n",
    "        for i in mix_id:\n",
    "            xi = X[i,:].reshape(1, d)   # dim(1,d)\n",
    "            yi = Y[i,:].reshape(1, C)   # dim(1,C)\n",
    "            ai = softmax(xi.dot(W[-1].T))   # dim(1,C)\n",
    "            W_new = W[-1] + eta*np.dot((yi - ai).T, xi) # dim(C,d)\n",
    "            # stopping criteria\n",
    "            if epoch % check_w_after == 0 and early_stopping == True:                \n",
    "                if np.linalg.norm(W_new - W[-check_w_after]) < tol:\n",
    "                    return W[-1], epoch\n",
    "            W.append(W_new)\n",
    "    return W[-1], epoch\n",
    "\n",
    "\n",
    "W, epoch = softmax_regression(X_train, y_train, 0.5, 1e-6)\n",
    "print('Coefficients found by built-in model:')\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients found by Sklearn model:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-12.76581404,  19.72232837],\n",
       "       [-11.9852142 , -43.6974649 ],\n",
       "       [ 24.75102825,  23.97513653]])"
      ]
     },
     "execution_count": 1128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C = 1e5, solver = 'lbfgs', multi_class = 'multinomial', random_state = 0)\n",
    "model.fit(X, y.reshape(X.shape[0]))\n",
    "print('Coefficients found by Sklearn model:')\n",
    "model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 100.00 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3, 1, 3]), array([[3, 1, 3]]))"
      ]
     },
     "execution_count": 1129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate models:\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)*100:.2f} %')\n",
    "y_pred, y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 100.00 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3, 1, 3]), array([[3, 1, 3]]))"
      ]
     },
     "execution_count": 1130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_ = W\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'Accuracy score: {accuracy_score(y_test, y_pred)*100:.2f} %')\n",
    "y_pred, y_test.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
